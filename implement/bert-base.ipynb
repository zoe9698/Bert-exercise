{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "lasting-account",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "all_punctuation = '!,;:?\"\\'、，；.()<>&$\\/\\`[\\]\\-\\—+£'\n",
    "sub_punctuation = '[.,?!]'\n",
    "def removePunctuation(text):\n",
    "    text = re.sub(r'[{}]+'.format(sub_punctuation),' ',text)\n",
    "    return text.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "instant-drunk",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 不要重复执行\n",
    "with open(\"D:\\\\MUST\\\\2020\\\\11.14\\\\MRPC\\\\train.tsv\",'r',encoding='utf-8') as f:\n",
    "    lines = f.readlines()\n",
    " \n",
    "text = []\n",
    "for line in lines[1:]:\n",
    "    label = line.split(\"\\t\")[0]\n",
    "    txt_before = line.split(\"\\t\")[3]\n",
    "    txt_after = line.split(\"\\t\")[4]\n",
    "    text.append([label,txt_before,txt_after])\n",
    "text\n",
    "with open(\"D:\\\\MUST\\\\2021\\\\MRPCtrain.txt\",'w',encoding='utf-8') as f:\n",
    "    for line in text:\n",
    "        f.write(\"\\t\".join(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "cutting-prior",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"D:\\\\MUST\\\\2021\\\\MRPCtrain.txt\",'r',encoding='utf-8') as f:\n",
    "    lines = f.readlines() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "alpha-acoustic",
   "metadata": {},
   "outputs": [],
   "source": [
    "#数据集大小\n",
    "dataset_size = len(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "infrared-spelling",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_list = []\n",
    "beforetxt_list = []\n",
    "aftertxt_list = []\n",
    "for line in lines[:dataset_size]:\n",
    "    line = line.split('\\t')\n",
    "    label_list.append(line[0])\n",
    "    beforetxt_list.append(line[1])\n",
    "    aftertxt_list.append(line[2].replace(\"\\n\",\"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "hundred-disorder",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import math\n",
    "import torch\n",
    "import numpy as np\n",
    "from random import *\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "general-antibody",
   "metadata": {},
   "outputs": [],
   "source": [
    "alltxt_list = beforetxt_list+aftertxt_list\n",
    "alltxt = \" \".join(alltxt_list)\n",
    "#去掉所有句子的标点符号\n",
    "sentences = re.sub(\"[\\\"$\\'\\’\\-\\(\\)%.,?!]\",'',alltxt.lower()).split('\\n')\n",
    "#所有词的列表\n",
    "word_list = list(set(\" \".join(sentences).split()))\n",
    "#生成带索引的词字典，并添加上四种标志词\n",
    "word2idx = {'[PAD]':0,'[CLS]':1,'[SEP]':2,'[MASK]':3}\n",
    "for i,w in enumerate(word_list):\n",
    "    word2idx[w] = i+4\n",
    "# 对换word2idx的key和value->idx2word\n",
    "idx2word = dict(zip(word2idx.values(),word2idx.keys()))\n",
    "# 上面的两个字典的大小\n",
    "vocab_size = len(idx2word)\n",
    "# token是所有句子的idx表示，每个句子一个list，所有句子组成一个大list\n",
    "token_list = list()\n",
    "for sentence in alltxt_list:\n",
    "    sentence = re.sub('[\\\"$\\'\\’\\-\\(\\)%.,?!]','',sentence.lower())\n",
    "    idx_sentence = [word2idx[s] for s in sentence.split()]\n",
    "    token_list.append(idx_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "macro-provision",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BERT Parameters\n",
    "\n",
    "# maxlen表示同一个batch中的所有句子都由30个token组成，不够的补PAD\n",
    "# （这里我实现的方式比较粗暴，直接固定所有batch中的所有句子都为30）\n",
    "maxlen = 100\n",
    "batch_size = 6\n",
    "# max_pred表示最多需要预测多少个单词，即BERT中的完形填空任务\n",
    "max_pred = 8 \n",
    "# n_layers表示Encoder Layer的数量\n",
    "n_layers = 6\n",
    "n_heads = 12\n",
    "# d_model表示Token Embeddings、Segment Embeddings、Position Embeddings的维度\n",
    "d_model = 768\n",
    "# d_ff表示Encoder Layer中全连接层的维度\n",
    "d_ff = 768*4 \n",
    "d_k = d_v = 64  # dimension of K(=Q), V\n",
    "# n_segments表示Decoder input由几句话组成\n",
    "n_segments = 2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "breeding-honduras",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_data():\n",
    "    i = 0\n",
    "    batchs = []\n",
    "    positive = negative = 0\n",
    "    while positive!=batch_size/2 or negative!=batch_size/2:\n",
    "        print(\"[\",i,\"]\")\n",
    "        i+=1\n",
    "        #从sentences中随机取两句话的序号\n",
    "        tokens_a_index = randrange(dataset_size)\n",
    "        tokens_b_index = tokens_a_index+dataset_size\n",
    "        print(tokens_a_index,tokens_b_index,int(label_list[tokens_a_index]))\n",
    "        #取对应的句子的token（idx_list）\n",
    "        tokens_a,tokens_b = token_list[tokens_a_index],token_list[tokens_b_index]\n",
    "        #将两个句子合并，并且加上开头结尾符号和中间分隔符，得到一个list，input_tokens\n",
    "        input_tokens = [word2idx['[CLS]']]+tokens_a+[word2idx['[SEP]']]+tokens_b+[word2idx['[SEP]']]\n",
    "        #segment_tokens表示句子前后顺序\n",
    "        segment_tokens = [0]*(1+len(tokens_a)+1)+[1]*(len(tokens_b)+1)\n",
    "        #n_pred(编码器需要预测的也就是被遮盖住的词的个数)按原文要求取单个训练例子（两个句子结合的句子）长度的15%遮盖住，限定必须小于max_pred\n",
    "        #n_pred = min(max_pred,len(input_index)*0.15)\n",
    "        n_pred = int(len(input_tokens)*0.15)\n",
    "\n",
    "        #指代了真实单词的位置，也就是可以被mask的单词的位置\n",
    "        cand_masked_pos = [i for i,token in enumerate(input_tokens)\n",
    "                          if token!=word2idx['[CLS]'] and token!=word2idx['[SEP]']]\n",
    "        \n",
    "        #将这个句子的所有单词的位置都打散\n",
    "        shuffle(cand_masked_pos)\n",
    "        #被遮住的词和被遮住的词的位置\n",
    "        masked_tokens,masked_pos = [],[]\n",
    "        #注：masked_tokens是原词，input_tokens中对应位置的是替换后的词\n",
    "        #取前n_pred个词，因为已经打散了，这就相当于随机去了n_pred个\n",
    "        for pos in cand_masked_pos[:n_pred]:\n",
    "            masked_pos.append(pos)\n",
    "            masked_tokens.append(input_tokens[pos])\n",
    "            if random()<0.8:\n",
    "                input_tokens[pos] = word2idx['[MASK]']\n",
    "            elif random()<0.1:\n",
    "                input_tokens[pos] = randint(4,vocab_size-1)\n",
    "        print(len(input_tokens))\n",
    "        print(len(segment_tokens))\n",
    "        #总句子不够长度的位置补[PAD]\n",
    "        if len(input_tokens)<maxlen:\n",
    "            input_tokens.extend([0]*(maxlen-len(input_tokens)))\n",
    "            segment_tokens.extend([0]*(maxlen-len(segment_tokens)))\n",
    "            #前面有写到n_pred<max_pred\n",
    "            #E.g. input_ids = [1,8, 36, 27, 13, 39, 33, 34,2,39, 33, 35, 26, 30, 38, 17, 5, 22, 16, 6, 12,2]\n",
    "            #     (shuffle)cand_maked_pos = [4, 17, 13, 5, 6, 14, 2, 15, 18, 20, 1, 3, 19, 7, 12, 16, 11, 9, 10]\n",
    "            #     len:19  ;  maxlen:30  ;   n_pred = 19*0.15 = 2.85 = 2  ;  max_pred = 5\n",
    "            #     masked_pos:[4,17]  masked_tokens:[13,22]\n",
    "            #     (masked)input_ids:[4->4, 17->4, 13, 5, 6, 14, 2, 15, 18, 20, 1, 3, 19, 7, 12, 16, 11, 9, 10]\n",
    "            #     masked_pos(补零):[4, 17, 0, 0, 0] masked_tokens(补零):[13,22,0,0,0]\n",
    "            masked_tokens.extend([0]*(max_pred-n_pred))\n",
    "            masked_pos.extend([0]*(max_pred-n_pred))\n",
    "        print(len(input_tokens))\n",
    "        print(len(segment_tokens))\n",
    "        #tokens_a_index + 3668 == tokens_b_index且label_list[tokens_a_index]==1\n",
    "#         :表示两个句子相邻，是上下文关系\n",
    "        #positive：表示随机抽到的句子对中，两个句子相邻，的次数\n",
    "        if label_list[tokens_a_index]=='1' and positive < batch_size/2:\n",
    "            print(\"positive\")\n",
    "            print(label_list[tokens_a_index])\n",
    "            batchs.append([input_tokens, segment_tokens, masked_tokens, masked_pos, True]) # IsNext\n",
    "            positive += 1\n",
    "        elif label_list[tokens_a_index]=='0' and negative < batch_size/2:\n",
    "            print(\"negative\")\n",
    "            print(label_list[tokens_a_index])\n",
    "            batchs.append([input_tokens, segment_tokens, masked_tokens, masked_pos, False]) # NotNext\n",
    "            negative += 1\n",
    "      \n",
    "    print(i,positive,negative)\n",
    "    return batchs    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "canadian-drinking",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0 ]\n",
      "973 4641 0\n",
      "35\n",
      "35\n",
      "100\n",
      "100\n",
      "negative\n",
      "0\n",
      "[ 1 ]\n",
      "1236 4904 1\n",
      "54\n",
      "54\n",
      "100\n",
      "100\n",
      "positive\n",
      "1\n",
      "[ 2 ]\n",
      "2972 6640 0\n",
      "27\n",
      "27\n",
      "100\n",
      "100\n",
      "negative\n",
      "0\n",
      "[ 3 ]\n",
      "2814 6482 0\n",
      "51\n",
      "51\n",
      "100\n",
      "100\n",
      "negative\n",
      "0\n",
      "[ 4 ]\n",
      "1431 5099 1\n",
      "55\n",
      "55\n",
      "100\n",
      "100\n",
      "positive\n",
      "1\n",
      "[ 5 ]\n",
      "3404 7072 0\n",
      "35\n",
      "35\n",
      "100\n",
      "100\n",
      "[ 6 ]\n",
      "851 4519 1\n",
      "35\n",
      "35\n",
      "100\n",
      "100\n",
      "positive\n",
      "1\n",
      "7 3 3\n"
     ]
    }
   ],
   "source": [
    "batchs = make_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "elect-alabama",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids, segment_ids, masked_tokens, masked_pos, isNext = zip(*batchs)\n",
    "#转换为torch张量\n",
    "input_ids, segment_ids, masked_tokens, masked_pos, isNext = \\\n",
    "    torch.LongTensor(input_ids),  torch.LongTensor(segment_ids), torch.LongTensor(masked_tokens),\\\n",
    "    torch.LongTensor(masked_pos), torch.LongTensor(isNext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "curious-armenia",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[    1,     3,   636,  8008,  2549,     3,  5452, 11061,  5178, 12574,\n",
       "           2359,  5993,     2,  3464,     3,  1575,   636,  4992,  4891, 11061,\n",
       "           5178,  5452,  2148,     3, 12574,  2359,  5993,     2,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
       "         [    1,     3,  2184,  2757,  7028,     3,  1873, 11726,  5155, 12401,\n",
       "           6343,  4367,  6343, 13374, 10295,  6798,  9159,  2713, 11061,     3,\n",
       "              2,  4291,     3,  7795,  3464,  9494,  6343,     3,  4367,  2757,\n",
       "           5452,  1868,  1873, 11726,  5155, 12401,  6343,  4367,  6343, 13374,\n",
       "          10295,     2,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
       "         [    1,  8489,  3579, 11959,  9128,  7861,  3464,   592,  6343, 11398,\n",
       "          10825,  3464,     3,  1575, 12831,   988,  7871,  5452,  2370,  3464,\n",
       "          13129,  5302, 12574,  1438,  1103,  7164,     2,     3,     3, 11959,\n",
       "           9128,  5396,     3, 12477, 12777, 10898,  3464,   592,  6343, 11398,\n",
       "          10825,  3464,  7457,     3, 12831,   988,  7871,  5452,  2370,  3464,\n",
       "          13129,  5302, 12574,  1438,  1103,  7164,     2,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
       "         [    1,  1770,  2671,  8489,  8916,     3,     3,  8615, 12174,     3,\n",
       "           7575,  6113,     2,  3464,  8808,  2671,  8489,  8916,  8615, 12174,\n",
       "          12618,  3579,  9877,     3,  5452,  3464,  2162,     2,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
       "         [    1,  6302,  7428,  2520,  6020, 11902,  9609, 13173, 12184, 12574,\n",
       "          12368,   710,  1766,     3, 11571,     3, 11685,     3, 12618,  3453,\n",
       "           3132,  4899,  1072,  8489,   437, 12318,  7795,   465,     2,     3,\n",
       "           7428,  2520,     3, 13558, 13252,  5452,    15,  4021,  2251, 11685,\n",
       "          12174, 12618,  1072,     3,   437,     3,  4053,  3464,  3453,  3132,\n",
       "              3,  4072,  1575,  5881,     2,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0],\n",
       "         [    1,  5333,     3,  3579,  6198, 12785, 12174,  8489,  3464,  1793,\n",
       "              3,  3579, 13157,  6020,  5378,  8084,  6379,  9650, 13052,  2238,\n",
       "           3464,  5333,  8000,  6113,     2,  5333,     3,   345,  3579,  6198,\n",
       "          12785,  8489,  3464,  1793,     3, 13157, 12299,  9813,  4592,  5647,\n",
       "              3,  8000,  2278,     3,     3, 12174,     2,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "              0,     0,     0,     0,     0,     0,     0,     0,     0,     0]]),\n",
       " tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0],\n",
       "         [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0]]),\n",
       " tensor([[ 5023,  5966,  3464, 11803,     0,     0,     0,     0],\n",
       "         [ 6343,  8696,  7463, 12556,  2902,  3012,     0,     0],\n",
       "         [ 7457, 13129,  3464,  1575,  8489, 11398, 11178,  3464],\n",
       "         [ 9119,  3464,  6246,  2708,     0,     0,     0,     0],\n",
       "         [12174,  6302,  8489,  6020,  4899, 12318,  1575,  2251],\n",
       "         [ 6113,   345,  5333,  6343,  6343, 11177, 13026,     0]]),\n",
       " tensor([[14,  5,  1, 23,  0,  0,  0,  0],\n",
       "         [12,  1, 27, 19,  5, 22,  0,  0],\n",
       "         [12, 20, 28, 43, 27,  9, 32,  6],\n",
       "         [ 6,  9,  5, 23,  0,  0,  0,  0],\n",
       "         [17, 29, 43, 32, 50, 45, 13, 15],\n",
       "         [44,  2, 40, 10, 34, 43, 26,  0]]),\n",
       " tensor([0, 1, 1, 0, 1, 0]))"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids, segment_ids, masked_tokens, masked_pos, isNext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "moral-cyprus",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataSet(Data.Dataset):\n",
    "  def __init__(self, input_ids, segment_ids, masked_tokens, masked_pos, isNext):\n",
    "    self.input_ids = input_ids\n",
    "    self.segment_ids = segment_ids\n",
    "    self.masked_tokens = masked_tokens\n",
    "    self.masked_pos = masked_pos\n",
    "    self.isNext = isNext\n",
    "  \n",
    "  def __len__(self):\n",
    "    return len(self.input_ids)\n",
    "  \n",
    "  def __getitem__(self, idx):\n",
    "    return self.input_ids[idx], self.segment_ids[idx], self.masked_tokens[idx], self.masked_pos[idx], self.isNext[idx]\n",
    "\n",
    "loader = Data.DataLoader(MyDataSet(input_ids, segment_ids, masked_tokens, masked_pos, isNext), batch_size, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sweet-barcelona",
   "metadata": {},
   "source": [
    "数据预处理部分结束"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "smoking-breathing",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedding(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Embedding,self).__init__()\n",
    "        self.tok_embed = nn.Embedding(vocab_size,d_model)#对总词典进行embedding编码\n",
    "        self.pos_embed = nn.Embedding(maxlen,d_model)#对每个位置进行embedding编码\n",
    "        self.seg_embed = nn.Embedding(n_segments,d_model)#对segment的值0/1进行embedding编码\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        \n",
    "    def forward(self,input_tokens,input_segments):\n",
    "        #input_x：是一个batch=6个句子对，每个句子对的所有词的idx组成一个list，即二维\n",
    "        #input_pos：每个句子对的位置索引list，没有传入，在下面这三行生成\n",
    "        #下面这三行相当于把pos(input)扩展成和input_x,input_segment一样的shape\n",
    "        #1.第二维大小=maxlen=100\n",
    "        seq_len = maxlen\n",
    "        #2.取这个batch中所有位置索引[0,1,2...,maxlen-1]\n",
    "        seq_pos = torch.arange(seq_len,dtype=torch.long)\n",
    "        #seq_pos = torch.arange(seq_len,dtype=torch.int8)\n",
    "        \n",
    "        #3.unsqueeze在第一个维度前增加一个维度，E.g.shape[100]->shape[1,100]\n",
    "        #expand_as将seq_pos扩展成和input_tokens一样的shape[6,100]\n",
    "        seq_pos = seq_pos.unsqueeze(0).expand_as(input_tokens)\n",
    "        \n",
    "        \n",
    "        #调用自带的embedding函数，分别对input_tokens,seq_pos,input_segments生成三个embed值，累加\n",
    "        embedding = self.tok_embed(input_tokens)+self.pos_embed(seq_pos)+self.seg_embed(input_segments)\n",
    "        #归一化\n",
    "        return self.norm(embedding)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "sexual-explosion",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids: torch.Size([6, 100])\n",
      "\n",
      "segment_ids: torch.Size([6, 100])\n",
      "\n",
      "masked_tokens: torch.Size([6, 8])\n",
      "\n",
      "masked_pos: torch.Size([6, 8])\n",
      "\n",
      "isNext: torch.Size([6])\n",
      "\n",
      "t_e_size: torch.Size([6, 100, 768])\n",
      "\n",
      "t_e: tensor([[[-1.6172,  0.3280,  0.9971,  ...,  0.0850, -0.1743,  0.6148],\n",
      "         [ 0.5803,  1.9750,  0.5276,  ..., -0.8609, -0.2403, -0.3550],\n",
      "         [ 0.6973, -0.5578, -0.0442,  ...,  0.2790, -1.1133,  0.7884],\n",
      "         ...,\n",
      "         [ 0.5494, -0.1238,  1.7511,  ...,  0.4101, -0.5980, -2.2431],\n",
      "         [ 0.5494, -0.1238,  1.7511,  ...,  0.4101, -0.5980, -2.2431],\n",
      "         [ 0.5494, -0.1238,  1.7511,  ...,  0.4101, -0.5980, -2.2431]],\n",
      "\n",
      "        [[-1.6172,  0.3280,  0.9971,  ...,  0.0850, -0.1743,  0.6148],\n",
      "         [-1.2548,  0.1464, -0.4946,  ...,  0.1805, -1.2416,  0.2647],\n",
      "         [ 0.5803,  1.9750,  0.5276,  ..., -0.8609, -0.2403, -0.3550],\n",
      "         ...,\n",
      "         [ 0.5494, -0.1238,  1.7511,  ...,  0.4101, -0.5980, -2.2431],\n",
      "         [ 0.5494, -0.1238,  1.7511,  ...,  0.4101, -0.5980, -2.2431],\n",
      "         [ 0.5494, -0.1238,  1.7511,  ...,  0.4101, -0.5980, -2.2431]],\n",
      "\n",
      "        [[-1.6172,  0.3280,  0.9971,  ...,  0.0850, -0.1743,  0.6148],\n",
      "         [-0.0344, -0.3595,  0.2561,  ...,  1.4388, -0.1444,  1.5118],\n",
      "         [-1.5632, -0.8057,  0.6447,  ...,  0.3536,  0.2454,  0.4047],\n",
      "         ...,\n",
      "         [ 0.5494, -0.1238,  1.7511,  ...,  0.4101, -0.5980, -2.2431],\n",
      "         [ 0.5494, -0.1238,  1.7511,  ...,  0.4101, -0.5980, -2.2431],\n",
      "         [ 0.5494, -0.1238,  1.7511,  ...,  0.4101, -0.5980, -2.2431]],\n",
      "\n",
      "        [[-1.6172,  0.3280,  0.9971,  ...,  0.0850, -0.1743,  0.6148],\n",
      "         [ 0.3166, -0.5737,  0.0674,  ...,  0.9128,  0.0153, -1.0379],\n",
      "         [-1.6038,  0.6567,  1.4829,  ..., -0.4890, -0.2498,  0.8431],\n",
      "         ...,\n",
      "         [ 0.5494, -0.1238,  1.7511,  ...,  0.4101, -0.5980, -2.2431],\n",
      "         [ 0.5494, -0.1238,  1.7511,  ...,  0.4101, -0.5980, -2.2431],\n",
      "         [ 0.5494, -0.1238,  1.7511,  ...,  0.4101, -0.5980, -2.2431]],\n",
      "\n",
      "        [[-1.6172,  0.3280,  0.9971,  ...,  0.0850, -0.1743,  0.6148],\n",
      "         [-0.9210, -1.1092,  1.6035,  ..., -0.7843,  1.6318,  0.8215],\n",
      "         [-0.3148,  0.8008,  0.1705,  ...,  0.5313, -0.4901,  1.1620],\n",
      "         ...,\n",
      "         [ 0.5494, -0.1238,  1.7511,  ...,  0.4101, -0.5980, -2.2431],\n",
      "         [ 0.5494, -0.1238,  1.7511,  ...,  0.4101, -0.5980, -2.2431],\n",
      "         [ 0.5494, -0.1238,  1.7511,  ...,  0.4101, -0.5980, -2.2431]],\n",
      "\n",
      "        [[-1.6172,  0.3280,  0.9971,  ...,  0.0850, -0.1743,  0.6148],\n",
      "         [ 0.5803,  1.9750,  0.5276,  ..., -0.8609, -0.2403, -0.3550],\n",
      "         [-0.1261, -0.5666, -1.2510,  ..., -0.6288,  0.3630, -1.0793],\n",
      "         ...,\n",
      "         [ 0.5494, -0.1238,  1.7511,  ...,  0.4101, -0.5980, -2.2431],\n",
      "         [ 0.5494, -0.1238,  1.7511,  ...,  0.4101, -0.5980, -2.2431],\n",
      "         [ 0.5494, -0.1238,  1.7511,  ...,  0.4101, -0.5980, -2.2431]]],\n",
      "       grad_fn=<EmbeddingBackward>)\n",
      "\n",
      "output_size: torch.Size([6, 100, 768])\n",
      "\n",
      "output: tensor([[[ 1.5776, -0.5242, -1.0730,  ...,  0.2922, -1.1166, -0.6197],\n",
      "         [ 0.9846,  0.9810, -1.2893,  ..., -0.1131, -0.3950,  1.0511],\n",
      "         [-0.5629,  0.9529,  0.5149,  ...,  0.2788, -0.6484, -0.5116],\n",
      "         ...,\n",
      "         [-0.4870, -0.3062, -1.0473,  ..., -0.2706, -0.5511,  0.8736],\n",
      "         [-1.3456,  0.4670, -0.5636,  ...,  1.1179,  0.5196,  1.1983],\n",
      "         [-0.4092, -0.7309, -0.9033,  ...,  0.7035, -0.5920, -0.5229]],\n",
      "\n",
      "        [[ 1.5776, -0.5242, -1.0730,  ...,  0.2922, -1.1166, -0.6197],\n",
      "         [ 0.8265,  1.9962, -0.5693,  ...,  0.3943, -1.2347, -0.5810],\n",
      "         [ 0.2071,  1.9480, -0.6310,  ...,  0.5999, -1.1654,  0.0738],\n",
      "         ...,\n",
      "         [-0.4870, -0.3062, -1.0473,  ..., -0.2706, -0.5511,  0.8736],\n",
      "         [-1.3456,  0.4670, -0.5636,  ...,  1.1179,  0.5196,  1.1983],\n",
      "         [-0.4092, -0.7309, -0.9033,  ...,  0.7035, -0.5920, -0.5229]],\n",
      "\n",
      "        [[ 1.5776, -0.5242, -1.0730,  ...,  0.2922, -1.1166, -0.6197],\n",
      "         [ 0.3922,  1.2187, -0.3753,  ...,  0.3608, -1.0732,  1.3438],\n",
      "         [-0.2031,  1.4136, -0.2975,  ...,  2.6575, -0.8268, -1.2568],\n",
      "         ...,\n",
      "         [-0.4870, -0.3062, -1.0473,  ..., -0.2706, -0.5511,  0.8736],\n",
      "         [-1.3456,  0.4670, -0.5636,  ...,  1.1179,  0.5196,  1.1983],\n",
      "         [-0.4092, -0.7309, -0.9033,  ...,  0.7035, -0.5920, -0.5229]],\n",
      "\n",
      "        [[ 1.5776, -0.5242, -1.0730,  ...,  0.2922, -1.1166, -0.6197],\n",
      "         [ 0.4751,  0.8285, -0.3845,  ...,  1.0721, -0.5614,  0.0840],\n",
      "         [ 0.9554,  1.3085, -0.5408,  ...,  2.4195, -1.5368, -0.1057],\n",
      "         ...,\n",
      "         [-0.4870, -0.3062, -1.0473,  ..., -0.2706, -0.5511,  0.8736],\n",
      "         [-1.3456,  0.4670, -0.5636,  ...,  1.1179,  0.5196,  1.1983],\n",
      "         [-0.4092, -0.7309, -0.9033,  ...,  0.7035, -0.5920, -0.5229]],\n",
      "\n",
      "        [[ 1.5776, -0.5242, -1.0730,  ...,  0.2922, -1.1166, -0.6197],\n",
      "         [ 0.6217,  0.6209, -1.1520,  ...,  1.7642,  0.0520,  0.2154],\n",
      "         [ 0.3641,  2.1357, -0.5361,  ...,  1.8595, -0.7254, -0.6213],\n",
      "         ...,\n",
      "         [-0.4870, -0.3062, -1.0473,  ..., -0.2706, -0.5511,  0.8736],\n",
      "         [-1.3456,  0.4670, -0.5636,  ...,  1.1179,  0.5196,  1.1983],\n",
      "         [-0.4092, -0.7309, -0.9033,  ...,  0.7035, -0.5920, -0.5229]],\n",
      "\n",
      "        [[ 1.5776, -0.5242, -1.0730,  ...,  0.2922, -1.1166, -0.6197],\n",
      "         [ 0.9846,  0.9810, -1.2893,  ..., -0.1131, -0.3950,  1.0511],\n",
      "         [ 0.0365,  1.3715, -0.6978,  ...,  1.7500, -1.5482, -0.4663],\n",
      "         ...,\n",
      "         [-0.4870, -0.3062, -1.0473,  ..., -0.2706, -0.5511,  0.8736],\n",
      "         [-1.3456,  0.4670, -0.5636,  ...,  1.1179,  0.5196,  1.1983],\n",
      "         [-0.4092, -0.7309, -0.9033,  ...,  0.7035, -0.5920, -0.5229]]],\n",
      "       grad_fn=<NativeLayerNormBackward>)\n"
     ]
    }
   ],
   "source": [
    "embedding = Embedding()\n",
    "tok_embed = nn.Embedding(vocab_size,d_model)\n",
    "\n",
    "i = 0\n",
    "for input_tokens, segment_tokens, masked_tokens, masked_pos, isNext in loader:\n",
    "    if i==1:\n",
    "        break\n",
    "    print(\"input_ids:\",input_tokens.size())\n",
    "    print(\"\\nsegment_ids:\",segment_tokens.size())\n",
    "    print(\"\\nmasked_tokens:\",masked_tokens.size())\n",
    "    print(\"\\nmasked_pos:\",masked_pos.size())\n",
    "    print(\"\\nisNext:\",isNext.size())\n",
    "    i+=1\n",
    "    t_e = tok_embed(input_tokens)\n",
    "    print(\"\\nt_e_size:\",t_e.size())\n",
    "    print(\"\\nt_e:\",t_e)\n",
    "    output = embedding(input_tokens,segment_tokens)\n",
    "    print(\"\\noutput_size:\",output.size())\n",
    "    print(\"\\noutput:\",output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incorporated-necessity",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_attn_pad_mask(input_tokens_q,input_tokens_k):\n",
    "    batch_size,seq_len = input_tokens_q.size()\n",
    "    pad_attn_mask = input_tokens_q.data.eq(0).unsqueeze(1)\n",
    "    return pad_attn_mask.expand(batch_size,seq_len,seq_len)#[6,100,100]全是True/False\n",
    "\n",
    "def gelu(x):\n",
    "    return x*0.5*(1.0+torch.erf(x/math.sqrt(2.0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rough-riverside",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaledDotProductAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ScaledDotProductAttention,self).__init__()\n",
    "        \n",
    "    def forward(self,Q,K,V,attn_mask):\n",
    "        scores = torch.matmul(Q,K.transpose(-1,-2))/np.sqrt(d_k)\n",
    "        scores.masked_fill_(attn_mask,-1e9)# Fills elements of self tensor with value where mask is 1或者True.\n",
    "        attn = nn.Softmax(dim=-1)(scores)\n",
    "        context = torch.matmul(attn,V)\n",
    "        return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "varied-journal",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MultiHeadAttention,self).__init__()\n",
    "        self.L_Q = nn.Linear(d_model,d_k*n_heads)\n",
    "        self.L_K = nn.Linear(d_model,d_k*n_heads)\n",
    "        self.L_V = nn.Linear(d_model,d_v*n_heads)\n",
    "        \n",
    "    def forward(self,Q,K,V,attn_mask):\n",
    "        #[6, 100, 12, 64]-->[6, 12, 100, 64]\n",
    "        q_s = self.L_Q(Q).view(batch_size, -1, n_heads, d_k).transpose(1,2)  # q_s: [batch_size, n_heads, seq_len, d_k]\n",
    "        k_s = self.L_K(K).view(batch_size, -1, n_heads, d_k).transpose(1,2)  # k_s: [batch_size, n_heads, seq_len, d_k]\n",
    "        v_s = self.L_V(V).view(batch_size, -1, n_heads, d_v).transpose(1,2)  # v_s: [batch_size, n_heads, seq_len, d_v]\n",
    "        \n",
    "        #attn_mask.unsqueeze(1).repeat(1,12,1,1)==attn_mask.unsqueeze(1).expand(6,12,100,100)\n",
    "        attn_mask = attn_mask.unsqueeze(1).repeat(1,n_heads,1,1)\n",
    "        #attn_mask = attn_mask.unsqueeze(1).expand(6,12,100,100)\n",
    "        \n",
    "        context = ScaledDotProductAttention()(q_s, k_s, v_s, attn_mask)\n",
    "        #####原Transformer论文中的CONCAT????是这里吗？\n",
    "        context = context.transpose(1,2).contiguous().view(batch_size,-1, n_heads * d_v)\n",
    "        #####原Transformer论文中的Linear\n",
    "        output = nn.Linear(n_heads * d_v, d_model)(context)\n",
    "        #####原Transformer论文中的每一个Multiheadattention后都跟了一个layernorm\n",
    "        return nn.LayerNorm(d_model)(output + residual) # output: [batch_size, seq_len, d_model]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "departmental-maine",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PoswiseFeedForwardNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PoswiseFeedForwardNet,self).__init__()\n",
    "        self.fc1 = nn.Linear(d_model,d_ff)\n",
    "        self.fc2 = nn.Linear(d_ff,d_model)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        return self.fc2(gelu(self.fc1(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "convertible-transfer",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EncoderLayer,self).__init__()\n",
    "        self.enc_self_attn = MultiHeadAttention()\n",
    "        self.pos_ffn = PoswiseFeedForwardNet()\n",
    "        \n",
    "    def forward(self,embedded_input_tokens,pad_attn_mask):\n",
    "        enc_outputs = self.enc_self_attn(embedded_input_tokens,embedded_input_tokens,embedded_input_tokens,pad_attn_mask)\n",
    "        enc_outputs = self.pos_ffn(enc_outputs)\n",
    "        return enc_outputs        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "naughty-anxiety",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected EOF while parsing (<ipython-input-123-68ff90a654ec>, line 11)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-123-68ff90a654ec>\"\u001b[1;36m, line \u001b[1;32m11\u001b[0m\n\u001b[1;33m    for layer in self.layers:\u001b[0m\n\u001b[1;37m                             ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m unexpected EOF while parsing\n"
     ]
    }
   ],
   "source": [
    "class BERT(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BERT,self).__init__()\n",
    "        self.embedding = Embedding()\n",
    "        self.layers = nn.ModuleList([EncoderLayer() for _ in range(n_layers)])#创建n_layers个encoderlayer层\n",
    "#         未完！！！！！！！   \n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(d_model,d_model),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "        self.classifier = nn.Linear(d_model,2)\n",
    "        self.linear = nn.Linear(d_model,d_model)\n",
    "        self.activ2 = gelu\n",
    "        embed_weight = self.embedding.tok_embed.weight\n",
    "        self.fc2 = nn.Linear(d_model,vocab_size,bias=False)\n",
    "        self.fc2.weight = embed_weight\n",
    "        \n",
    "    def forward(self,input_tokens,segment_tokens,masked_pos):\n",
    "        embedded_input_tokens = self.embedding(input_tokens,segment_tokens)\n",
    "        pad_attn_mask = get_attn_pad_mask(input_tokens,input_tokens)\n",
    "        for layer in self.layers:\n",
    "            embedded_input_tokens = layer(embedded_input_tokens,pad_attn_mask)\n",
    "        enc_output = embedded_input_tokens\n",
    "        \n",
    "        h_pooled = self.fc(enc_output[:, 0]) # [batch_size, d_model]\n",
    "        logits_clsf = self.classifier(h_pooled) # [batch_size, 2] predict isNext\n",
    "\n",
    "#         未完！！！！！！！\n",
    "        masked_pos = masked_pos[:, :, None].expand(-1, -1, d_model) # [batch_size, max_pred, d_model]\n",
    "        h_masked = torch.gather(enc_output, 1, masked_pos) # masking position [batch_size, max_pred, d_model]\n",
    "        h_masked = self.activ2(self.linear(h_masked)) # [batch_size, max_pred, d_model]\n",
    "        logits_lm = self.fc2(h_masked) # [batch_size, max_pred, vocab_size]\n",
    "        return logits_lm, logits_clsf\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "private-onion",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
